"""
OpenFDA
DAG auto-generated by Astro Cloud IDE.
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from datetime import datetime, timedelta
import pandas as pd
import requests

# ====== CONFIG ======
GCP_PROJECT  = "meu-projeto-471401"      # e.g., "my-gcp-project"
BQ_DATASET   = "openfda"                    # e.g., "crypto"
BQ_TABLE     = "openfda_food_events"       # e.g., "bitcoin_history_hourly"
BQ_LOCATION  = "US"                        # dataset location: "US" or "EU"
GCP_CONN_ID  = "google_cloud_default"      # Airflow connection with a SA that can write to BQ
# ====================

# Function to generate query URL for a specific month and year
def generate_query_url(year, month):
    start_date = f"{year}{month:02d}01"
    # Calculate last day of the month
    if month == 12:
        next_month = 1
        next_year = year + 1
    else:
        next_month = month + 1
        next_year = year
    
    last_day = (datetime(next_year, next_month, 1) - timedelta(days=1)).day
    end_date = f"{year}{month:02d}{last_day:02d}"
    
    query = f"https://api.fda.gov/food/event.json?search=products.industry_code:07+AND+date_started:[{start_date}+TO+{end_date}]&count=reactions.exact"
    return query

# Function to fetch data from the API and save it to XCom
def fetch_openfda_data(**kwargs):
    from airflow.operators.python import get_current_context
    
    ti = kwargs['ti']
    context = get_current_context()
    execution_date = context['logical_date']
    year = execution_date.year
    month = execution_date.month

    query_url = generate_query_url(year, month)
    print(f"Fetching data from: {query_url}")
    
    response = requests.get(query_url)

    if response.status_code == 200:
        data = response.json()
        print(f"API Response: {data}")
        
        # Extract results from the API response
        if 'results' in data and data['results']:
            df = pd.DataFrame(data['results'])
            
            # Add time column with execution date as string
            df['time'] = execution_date.strftime('%Y-%m-%d')
            
            # Reorder columns to match schema: time, term, count
            df = df[['time', 'term', 'count']]
            
            print(f"Data fetched successfully: {len(df)} rows")
            print(df.info())
            print(df.head())
        else:
            print("No results found in API response")
            df = pd.DataFrame(columns=['time', 'term', 'count'])
    else:
        print(f"API request failed with status code: {response.status_code}")
        df = pd.DataFrame(columns=['time', 'term', 'count'])  # Return empty DataFrame if request fails

    # Push the DataFrame to XCom
    ti.xcom_push(key='openfda_data', value=df.to_dict())

def save_to_bigquery(**kwargs):
    import pandas as pd
    from google.cloud.exceptions import NotFound
    from google.cloud import bigquery

    ti = kwargs['ti']
    data_dict = ti.xcom_pull(task_ids='fetch_openfda_data', key='openfda_data')
    
    if not data_dict:
        print("No data to save to BigQuery")
        return
    
    df = pd.DataFrame.from_dict(data_dict)
    print(f"Data to save: {len(df)} rows")
    print(df.head())
    
    # Initialize BigQuery hook and get credentials
    hook = BigQueryHook(gcp_conn_id=GCP_CONN_ID, location=BQ_LOCATION)
    credentials = hook.get_credentials()
    
    # Create destination table reference
    destination_table = f"{GCP_PROJECT}.{BQ_DATASET}.{BQ_TABLE}"
    
    # Use pandas-gbq to save data to BigQuery
    try:
        # Try to save with append mode first (table exists)
        df.to_gbq(
            destination_table=f"{BQ_DATASET}.{BQ_TABLE}",
            project_id=GCP_PROJECT,
            if_exists="append",
            credentials=credentials,
            location=BQ_LOCATION,
            progress_bar=False
        )
        print(f"Table {destination_table} already exists. Data appended successfully.")
        
    except Exception as e:
        # If table doesn't exist, create it with replace mode
        if "Not found" in str(e) or "does not exist" in str(e):
            print(f"Table {destination_table} does not exist. Creating table and inserting data...")
            
            # Define table schema for better type control
            table_schema = [
                {"name": "time", "type": "STRING"},
                {"name": "term", "type": "STRING"},
                {"name": "count", "type": "INTEGER"},
            ]
            
            df.to_gbq(
                destination_table=f"{BQ_DATASET}.{BQ_TABLE}",
                project_id=GCP_PROJECT,
                if_exists="replace",
                credentials=credentials,
                location=BQ_LOCATION,
                table_schema=table_schema,
                progress_bar=False
            )
            print(f"Table {destination_table} created successfully.")
        else:
            print(f"Error saving to BigQuery: {e}")
            raise e
    
    print(f"Successfully saved {len(df)} rows to {destination_table}")

# Define the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'fetch_openfda_food_events_monthly',
    default_args=default_args,
    description='Retrieve OpenFDA Food Events data monthly',
    schedule='@monthly',
    start_date=datetime(2020, 11, 1),
    catchup=True,
    max_active_tasks=1
)

fetch_task = PythonOperator(
    task_id='fetch_openfda_data',
    python_callable=fetch_openfda_data,
    dag=dag,
)

save_data_task = PythonOperator(
    task_id='save_to_bigquery',    
    python_callable=save_to_bigquery,
    dag=dag,
)

fetch_task >> save_data_task

