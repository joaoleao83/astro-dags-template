"""
OpenFDA
DAG auto-generated by Astro Cloud IDE.
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from datetime import datetime, timedelta
import pandas as pd
import requests

# ====== CONFIG ======
GCP_PROJECT  = "meu-projeto-471401"      # e.g., "my-gcp-project"
BQ_DATASET   = "openfda"                    # e.g., "crypto"
BQ_TABLE     = "openfda_history"    # e.g., "bitcoin_history_hourly"
BQ_LOCATION  = "US"                        # dataset location: "US" or "EU"
GCP_CONN_ID  = "google_cloud_default"      # Airflow connection with a SA that can write to BQ
# ====================

# Function to generate query URL for a specific month and year
def generate_query_url(year, month):
    start_date = f"{year}{month:02d}01"
    end_date = f"{year}{month:02d}{(datetime(year, month, 1) + timedelta(days=31)).replace(day=1) - timedelta(days=1):%d}"
    query = f"https://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct:%22sildenafil+citrate%22+AND+receivedate:[{start_date}+TO+{end_date}]&count=receivedate"
    return query

# Function to fetch data from the API and save it to XCom
def fetch_openfda_data(**kwargs):
    from airflow.operators.python import get_current_context
    
    ti = kwargs['ti']
    context = get_current_context()
    execution_date = context['logical_date']
    year = execution_date.year
    month = execution_date.month

    query_url = generate_query_url(year, month)
    response = requests.get(query_url)

    if response.status_code == 200:
        data = response.json()
        df = pd.DataFrame(data['results'])
        df['time'] = pd.to_datetime(df['time'])
        # Group by week and sum the count column
        weekly_sum = df.groupby(pd.Grouper(key='time', freq='W'))['count'].sum().reset_index()
        weekly_sum["time"] = weekly_sum["time"].astype(str)
        print(weekly_sum.info())
        print(weekly_sum.head())
    else:
        weekly_sum = pd.DataFrame([])  # Return empty DataFrame if request fails

    # Push the DataFrame to XCom
    ti.xcom_push(key='openfda_data', value=weekly_sum.to_dict())

def save_to_bigquery(**kwargs):
    import pandas as pd
    from google.cloud.exceptions import NotFound

    ti = kwargs['ti']
    data_dict = ti.xcom_pull(task_ids='fetch_openfda_data', key='openfda_data')
    
    if not data_dict:
        print("No data to save to BigQuery")
        return
    
    df = pd.DataFrame.from_dict(data_dict)
    
    # Initialize BigQuery hook
    hook = BigQueryHook(gcp_conn_id=GCP_CONN_ID, location=BQ_LOCATION)
    
    # Get BigQuery client
    client = hook.get_client(project_id=GCP_PROJECT, location=BQ_LOCATION)
    
    # Define dataset and table references
    dataset_ref = client.dataset(BQ_DATASET)
    table_ref = dataset_ref.table(BQ_TABLE)
    destination_table = f"{BQ_DATASET}.{BQ_TABLE}"
    
    try:
        # Try to get the table to check if it exists
        table = client.get_table(table_ref)
        print(f"Table {GCP_PROJECT}.{destination_table} already exists. Appending data...")
        
        # Table exists, append data
        hook.insert_rows_from_dataframe(
            dataset_table=destination_table,
            dataframe=df,
            project_id=GCP_PROJECT,
            location=BQ_LOCATION,
            write_disposition="WRITE_APPEND"
        )
        
    except NotFound:
        print(f"Table {GCP_PROJECT}.{destination_table} does not exist. Creating table and inserting data...")
        
        # Table doesn't exist, create it with the data
        # Define table schema based on the DataFrame
        table_schema = [
            {"name": "time", "type": "STRING"},
            {"name": "count", "type": "INTEGER"},
        ]
        
        # Create table with schema and insert data
        hook.insert_rows_from_dataframe(
            dataset_table=destination_table,
            dataframe=df,
            project_id=GCP_PROJECT,
            location=BQ_LOCATION,
            write_disposition="WRITE_TRUNCATE",  # This will create the table if it doesn't exist
            table_schema=table_schema
        )
    
    print(f"Successfully saved {len(df)} rows to {GCP_PROJECT}.{destination_table}")

# Define the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'fetch_openfda_data_monthly',
    default_args=default_args,
    description='Retrieve OpenFDA data monthly',
    schedule='@monthly',
    start_date=datetime(2020, 11, 1),
    catchup=True,
    max_active_tasks=1
)


fetch_task = PythonOperator(
    task_id='fetch_openfda_data',
    python_callable=fetch_openfda_data,
    dag=dag,
)

save_data_task = PythonOperator(
    task_id='save_to_bigquery',    
    python_callable=save_to_bigquery,
    dag=dag,
)

fetch_task >> save_data_task

